# -*- coding: utf-8 -*-
"""CNN for Object Classification and Data Augmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ma3aJHriVHqboOXL7sunOLpUIsX4-qBC
"""

# Importing packages

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np

tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

# Loading cifar10 dataset
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalizing pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

# CNN model
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))

# Compiling the model
model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

# Training the model
model.fit(train_images, train_labels, epochs=10)

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(test_acc)

# Plotting the original image

plt.plot()
plt.imshow(train_images[0])
plt.show()

# Use any two augmentation techniques to improve the test accuracy. Should plot 
# one example of each augmentation technique. 
# 1. Random rotation
# 2. Random zoom

# 1. Random rotation
data_augmentation_rotate = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomRotation(0.2),
])

plt.plot()
plt.imshow(data_augmentation_rotate(train_images)[0])
plt.show()

# 2. Random zoom
data_augmentation_zoom = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomZoom(0.2),
])

plt.plot()
plt.imshow(data_augmentation_zoom(train_images)[0])
plt.show()

# Use the model to predict the label of the first 10 images in the test dataset.
# Print the predicted label and the true label for each image.

# Predict the label of the first 10 images in the test dataset
predictions = model.predict(test_images[:10])

# Print the predicted label and the true label for each image
for i in range(10):
    print("Predicted label: ", predictions[i].argmax(), "True label: ", test_labels[i])

# Expand the training dataset
train_images = np.concatenate((train_images, data_augmentation_rotate(train_images)), axis=0)
train_images = np.concatenate((train_images, data_augmentation_zoom(train_images)), axis=0)
train_labels = np.concatenate((train_labels, train_labels), axis=0)
train_labels = np.concatenate((train_labels, train_labels), axis=0)

# Train the model with the new data concatenated
model.fit(train_images, train_labels, epochs=10)

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(test_acc)

# Add dropout regularization to the model. Train the model again and evaluate the
# model. Report the test accuracy.

# CNN model
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))

# Compile the model
model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

# Training the model
model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(test_acc)

"""#### Part B - (3)

The test accuracy increased when adding data augmentation to the model. There can be numerous explanations for this increase. One explanation is that the data augmented data is concatenated to the original model for both random rotation and zoom augmentation. This triples the size of the data set as apparent in the epoch print-outs. By running the chosen data augmentation methods on the dataset, we artificially increase the amount of data present in our model. By randomly rotating and zooming each image, we are training the model harder and as a result, resulting in higher accuracy.

#### Part B - (4)

Using dropout regularization on the dense layers of the model showcased an increase in the validation and test accuracy. However, while training, the training accuracy was much lower compared to when the training was done without the dropout layer in the model. The amount of loss experienced with dropout is also higher when compared to without.

"""